{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "018b0cdb-1fba-4dcf-b6ec-9521d73dd5dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType, DateType\n",
    "from pyspark.sql.types import BooleanType\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acb8455b-8a1c-4827-be0c-d1628357247b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1577515130754796>:8\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m configs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.auth.type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOAuth\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.oauth.provider.type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morg.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.oauth2.client.id\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m78e4bb3d-4d36-4d5e-ab12-333ff6203b4a\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.oauth2.client.secret\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgkt8Q~_v2HAvSjkht9N4LMWhgMG-EICPDBgaYaAK\u001B[39m\u001B[38;5;124m'\u001B[39m,\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.oauth2.client.endpoint\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://login.microsoftonline.com/6b2da805-9a6f-4d40-a2c7-e59044cb18ec/oauth2/token\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n",
       "\u001B[0;32m----> 8\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mmount(\n",
       "\u001B[1;32m      9\u001B[0m source \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabfss://hrdata@hrdataproj.dfs.core.windows.net\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;66;03m# contrainer@storageacc\u001B[39;00m\n",
       "\u001B[1;32m     10\u001B[0m mount_point \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/hrdata\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     11\u001B[0m extra_configs \u001B[38;5;241m=\u001B[39m configs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    360\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    361\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m--> 362\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
       "\n",
       "\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o416.mount.\n",
       ": java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/hrdata; nested exception is: \n",
       "\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/hrdata\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1025)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1051)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:555)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:650)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:671)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:412)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:410)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:407)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:455)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:440)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:645)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:564)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:555)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:525)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1045)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/hrdata\n",
       "\tat scala.Predef$.require(Predef.scala:281)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:580)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:948)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:729)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:937)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:588)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:120)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$5(DbfsServerBackend.scala:387)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:208)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:387)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:331)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:668)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:686)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:208)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:663)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1037)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:948)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:540)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:418)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:208)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:54)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:418)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:175)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n",
       "\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:83)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:49)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:78)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n",
       "\t... 1 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-1577515130754796>:8\u001B[0m\n\u001B[1;32m      1\u001B[0m configs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.auth.type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOAuth\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.oauth.provider.type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morg.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.oauth2.client.id\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m78e4bb3d-4d36-4d5e-ab12-333ff6203b4a\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.oauth2.client.secret\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgkt8Q~_v2HAvSjkht9N4LMWhgMG-EICPDBgaYaAK\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.oauth2.client.endpoint\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://login.microsoftonline.com/6b2da805-9a6f-4d40-a2c7-e59044cb18ec/oauth2/token\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n\u001B[0;32m----> 8\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mmount(\n\u001B[1;32m      9\u001B[0m source \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabfss://hrdata@hrdataproj.dfs.core.windows.net\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;66;03m# contrainer@storageacc\u001B[39;00m\n\u001B[1;32m     10\u001B[0m mount_point \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/hrdata\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     11\u001B[0m extra_configs \u001B[38;5;241m=\u001B[39m configs)\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    360\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    361\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 362\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n\n\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o416.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/hrdata; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/hrdata\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1025)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1051)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:555)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:650)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:671)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:412)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:410)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:407)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:455)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:440)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:645)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:564)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:555)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:525)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1045)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/hrdata\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:580)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:948)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:729)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:937)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:588)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:120)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$5(DbfsServerBackend.scala:387)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:208)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:387)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:331)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:668)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:686)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:208)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:663)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1037)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:948)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:540)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:418)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:208)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:54)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:418)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:175)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:83)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:49)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:78)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n",
       "errorSummary": "java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/hrdata; nested exception is: ",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "\"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "\"fs.azure.account.oauth2.client.id\": \"78e4bb3d-4d36-4d5e-ab12-333ff6203b4a\",\n",
    "\"fs.azure.account.oauth2.client.secret\": 'gkt8Q~_v2HAvSjkht9N4LMWhgMG-EICPDBgaYaAK',\n",
    "\"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/6b2da805-9a6f-4d40-a2c7-e59044cb18ec/oauth2/token\"}\n",
    "\n",
    "\n",
    "dbutils.fs.mount(\n",
    "source = \"abfss://hrdata@hrdataproj.dfs.core.windows.net\", # contrainer@storageacc\n",
    "mount_point = \"/mnt/hrdata\",\n",
    "extra_configs = configs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a0b39a-7103-433a-8296-4d98d41f12fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/hrdata/raw-data/</td><td>raw-data/</td><td>0</td><td>1692988038000</td></tr><tr><td>dbfs:/mnt/hrdata/transformed-data/</td><td>transformed-data/</td><td>0</td><td>1692988135000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/hrdata/raw-data/",
         "raw-data/",
         0,
         1692988038000
        ],
        [
         "dbfs:/mnt/hrdata/transformed-data/",
         "transformed-data/",
         0,
         1692988135000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls \"/mnt/hrdata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af045313-375a-4ba9-9ed5-17761ec17684",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employee = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/mnt/hrdata/raw-data/Employee.csv\")\n",
    "action = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/mnt/hrdata/raw-data/Action.csv\")\n",
    "performance = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/mnt/hrdata/raw-data/performance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f098343-3772-4ff9-93fd-fb1dd6640b59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------------------+----------------+-------------+---------+-------+-----------+-------------+--------+-----+\n|employee_id|               name|date_of_employement|termination_date|department_id|gender_id|race_id|marriage_id|date_of_birth|pay_rate|level|\n+-----------+-------------------+-------------------+----------------+-------------+---------+-------+-----------+-------------+--------+-----+\n|          1|         Nash Kay K|           1/1/2015|       5/29/2017|            8|        1|      1|          1|    2/12/1987|    null|    1|\n|          2|  Simpson Raymond X|           1/1/2015|            null|            7|        0|      4|          2|   11/15/1982|    null|    1|\n|          3|       Devlin Kay M|          6/10/2015|       6/24/2019|           10|        1|      5|          3|    2/22/1986|    null|    1|\n|          4| Massie Elisabeth V|           1/1/2015|            null|            4|        1|      2|          4|    5/25/1977|    null|    1|\n|          5|   Mead Catherine N|         22/05/2015|       7/15/2020|            8|        1|      1|          1|     4/4/1995|    null|    2|\n|          6|     Wright Linda K|          11/7/2015|       6/18/2022|           10|        1|      2|          3|    2/13/1989|    null|    2|\n|          7|      Gibbs David A|          4/12/2015|       10/8/2018|            3|        0|      4|          7|     5/6/1983|    null|    1|\n|          8|      Shute Sarah F|           1/1/2015|            null|            9|        1|      2|          8|    4/11/1986|    null|    1|\n|          9|   Woolley Paulin P|           1/1/2015|       2/22/2015|            4|        0|      4|          4|     1/6/1995|    null|    2|\n|         10| Woodbridge Irene A|           9/5/2015|       6/15/2022|            1|        1|      5|         10|    6/21/1985|    null|    1|\n|         11|Richardson Bhavna C|          11/5/2015|       9/29/2021|            9|        1|      2|          8|    7/31/1978|    null|    2|\n|         12|      Lamb Daniel U|         24/12/2015|            null|            6|        0|      5|         12|     2/3/1975|    null|    1|\n|         13|   Seabrook Sonia S|           1/1/2015|            null|            1|        1|      2|         10|   12/17/1990|    null|    2|\n|         14|     Powell David V|         26/07/2015|            null|            6|        0|      5|         12|     5/2/1978|    null|    2|\n|         15|   Nielson Paulin V|         13/03/2016|            null|            4|        0|      2|          4|   11/19/1974|    null|    2|\n|         16|   Goddard Joanna R|           1/1/2015|            null|            7|        1|      5|          2|    8/27/1972|    null|    2|\n|         17|      Webb Steven C|          8/10/2015|            null|            3|        0|      2|          7|   10/26/1987|    null|    2|\n|         18| Channing Francis C|         21/03/2015|            null|            7|        0|      5|          2|    1/31/1968|    null|    2|\n|         19|    Richards Iris K|           1/1/2015|            null|            9|        1|      5|          8|    10/3/1968|    null|    2|\n|         20|     Ewart Amanda B|           1/1/2015|            null|            7|        1|      4|          2|    2/12/1981|    null|    2|\n+-----------+-------------------+-------------------+----------------+-------------+---------+-------+-----------+-------------+--------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51280251-931e-4d59-a1b8-60a9d383ec1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- date_of_employement: string (nullable = true)\n |-- termination_date: string (nullable = true)\n |-- department_id: string (nullable = true)\n |-- gender_id: string (nullable = true)\n |-- race_id: string (nullable = true)\n |-- marriage_id: string (nullable = true)\n |-- date_of_birth: string (nullable = true)\n |-- pay_rate: string (nullable = true)\n |-- level: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "employee.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "915e425b-f649-4154-8efc-5fb8adb857b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- date_of_employement: date (nullable = true)\n |-- termination_date: date (nullable = true)\n |-- department_id: integer (nullable = true)\n |-- gender_id: boolean (nullable = true)\n |-- race_id: integer (nullable = true)\n |-- marriage_id: integer (nullable = true)\n |-- date_of_birth: date (nullable = true)\n |-- pay_rate: integer (nullable = true)\n |-- level: integer (nullable = true)\n\n+-----------+-------------------+-------------------+----------------+-------------+---------+-------+-----------+-------------+--------+-----+\n|employee_id|               name|date_of_employement|termination_date|department_id|gender_id|race_id|marriage_id|date_of_birth|pay_rate|level|\n+-----------+-------------------+-------------------+----------------+-------------+---------+-------+-----------+-------------+--------+-----+\n|          1|         Nash Kay K|         2015-01-01|      2017-05-29|            8|     true|      1|          1|   1987-02-12|    null|    1|\n|          2|  Simpson Raymond X|         2015-01-01|            null|            7|    false|      4|          2|   1982-11-15|    null|    1|\n|          3|       Devlin Kay M|         2015-06-10|      2019-06-24|           10|     true|      5|          3|   1986-02-22|    null|    1|\n|          4| Massie Elisabeth V|         2015-01-01|            null|            4|     true|      2|          4|   1977-05-25|    null|    1|\n|          5|   Mead Catherine N|               null|      2020-07-15|            8|     true|      1|          1|   1995-04-04|    null|    2|\n|          6|     Wright Linda K|         2015-11-07|      2022-06-18|           10|     true|      2|          3|   1989-02-13|    null|    2|\n|          7|      Gibbs David A|         2015-04-12|      2018-10-08|            3|    false|      4|          7|   1983-05-06|    null|    1|\n|          8|      Shute Sarah F|         2015-01-01|            null|            9|     true|      2|          8|   1986-04-11|    null|    1|\n|          9|   Woolley Paulin P|         2015-01-01|      2015-02-22|            4|    false|      4|          4|   1995-01-06|    null|    2|\n|         10| Woodbridge Irene A|         2015-09-05|      2022-06-15|            1|     true|      5|         10|   1985-06-21|    null|    1|\n|         11|Richardson Bhavna C|         2015-11-05|      2021-09-29|            9|     true|      2|          8|   1978-07-31|    null|    2|\n|         12|      Lamb Daniel U|               null|            null|            6|    false|      5|         12|   1975-02-03|    null|    1|\n|         13|   Seabrook Sonia S|         2015-01-01|            null|            1|     true|      2|         10|   1990-12-17|    null|    2|\n|         14|     Powell David V|               null|            null|            6|    false|      5|         12|   1978-05-02|    null|    2|\n|         15|   Nielson Paulin V|               null|            null|            4|    false|      2|          4|   1974-11-19|    null|    2|\n|         16|   Goddard Joanna R|         2015-01-01|            null|            7|     true|      5|          2|   1972-08-27|    null|    2|\n|         17|      Webb Steven C|         2015-08-10|            null|            3|    false|      2|          7|   1987-10-26|    null|    2|\n|         18| Channing Francis C|               null|            null|            7|    false|      5|          2|   1968-01-31|    null|    2|\n|         19|    Richards Iris K|         2015-01-01|            null|            9|     true|      5|          8|   1968-10-03|    null|    2|\n|         20|     Ewart Amanda B|         2015-01-01|            null|            7|     true|      4|          2|   1981-02-12|    null|    2|\n+-----------+-------------------+-------------------+----------------+-------------+---------+-------+-----------+-------------+--------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Transform the data\n",
    "employee = employee \\\n",
    "    .withColumn(\"employee_id\", col(\"employee_id\").cast(IntegerType())) \\\n",
    "    .withColumn(\"department_id\", col(\"department_id\").cast(IntegerType())) \\\n",
    "    .withColumn(\"gender_id\", col(\"gender_id\").cast(BooleanType())) \\\n",
    "    .withColumn(\"race_id\", col(\"race_id\").cast(IntegerType())) \\\n",
    "    .withColumn(\"marriage_id\", col(\"marriage_id\").cast(IntegerType())) \\\n",
    "    .withColumn(\"pay_rate\", col(\"pay_rate\").cast(IntegerType())) \\\n",
    "    .withColumn(\"level\", col(\"level\").cast(IntegerType())) \\\n",
    "    .withColumn(\"date_of_employement\", to_date(col(\"date_of_employement\"), \"M/d/yyyy\")) \\\n",
    "    .withColumn(\"termination_date\", to_date(col(\"termination_date\"), \"M/d/yyyy\")) \\\n",
    "    .withColumn(\"date_of_birth\", to_date(col(\"date_of_birth\"), \"M/d/yyyy\"))\n",
    "\n",
    "# Print the schema after transformation\n",
    "employee.printSchema()\n",
    "\n",
    "# Show the data\n",
    "employee.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f8cca8c-4877-4716-8234-b008dad70036",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-----------+-----------------+\n|act_id|action_id|employee_id|date_of_effective|\n+------+---------+-----------+-----------------+\n|     1|       10|          1|         1/1/2015|\n|     2|       10|          2|         1/1/2015|\n|     3|       10|          3|        10/6/2015|\n|     4|       10|          4|         1/1/2015|\n|     5|       10|          5|        5/22/2015|\n|     6|       10|          6|        7/11/2015|\n|     7|       10|          7|        12/4/2015|\n|     8|       10|          8|         1/1/2015|\n|     9|       10|          9|         1/1/2015|\n|    10|       10|         10|         5/9/2015|\n|    11|       10|         11|        5/11/2015|\n|    12|       10|         12|       12/24/2015|\n|    13|       10|         13|         1/1/2015|\n|    14|       10|         14|        7/26/2015|\n|    15|       10|         15|        3/13/2016|\n|    16|       10|         16|         1/1/2015|\n|    17|       10|         17|        10/8/2015|\n|    18|       10|         18|        3/21/2015|\n|    19|       10|         19|         1/1/2015|\n|    20|       10|         20|         1/1/2015|\n+------+---------+-----------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "action.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1803d5cb-3b90-481d-bd82-8fde9bb809d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- act_id: string (nullable = true)\n |-- action_id: string (nullable = true)\n |-- employee_id: string (nullable = true)\n |-- date_of_effective: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "action.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b97371-421d-4179-85c3-abc4e825b81f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-----------+-----------------+\n|act_id|action_id|employee_id|date_of_effective|\n+------+---------+-----------+-----------------+\n|     1|       10|          1|         1/1/2015|\n|     2|       10|          2|         1/1/2015|\n|     3|       10|          3|        10/6/2015|\n|     4|       10|          4|         1/1/2015|\n|     5|       10|          5|        5/22/2015|\n|     6|       10|          6|        7/11/2015|\n|     7|       10|          7|        12/4/2015|\n|     8|       10|          8|         1/1/2015|\n|     9|       10|          9|         1/1/2015|\n|    10|       10|         10|         5/9/2015|\n|    11|       10|         11|        5/11/2015|\n|    12|       10|         12|       12/24/2015|\n|    13|       10|         13|         1/1/2015|\n|    14|       10|         14|        7/26/2015|\n|    15|       10|         15|        3/13/2016|\n|    16|       10|         16|         1/1/2015|\n|    17|       10|         17|        10/8/2015|\n|    18|       10|         18|        3/21/2015|\n|    19|       10|         19|         1/1/2015|\n|    20|       10|         20|         1/1/2015|\n+------+---------+-----------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "action.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bbaf9a3-9760-4207-8a40-fa49dc26c849",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- act_id: string (nullable = true)\n |-- action_id: string (nullable = true)\n |-- employee_id: string (nullable = true)\n |-- date_of_effective: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "action.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a8bdf24-777a-43f0-b834-f3b651e6527f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- performance_id: integer (nullable = true)\n |-- employee_id: integer (nullable = true)\n |-- rating: integer (nullable = true)\n |-- perfomance_date: date (nullable = true)\n\n+--------------+-----------+------+---------------+\n|performance_id|employee_id|rating|perfomance_date|\n+--------------+-----------+------+---------------+\n|             1|          1|     1|     2015-12-31|\n|             2|          2|     2|     2015-12-31|\n|             3|          3|     2|     2015-12-31|\n|             4|          4|     4|     2015-12-31|\n|             5|          5|     2|     2015-12-31|\n|             6|          6|     5|     2015-12-31|\n|             7|          7|     3|     2015-12-31|\n|             8|          8|     4|     2015-12-31|\n|             9|         10|     4|     2015-12-31|\n|            10|         11|     3|     2015-12-31|\n|            11|         12|     4|     2015-12-31|\n|            12|         13|     3|     2015-12-31|\n|            13|         14|     4|     2015-12-31|\n|            14|         16|     3|     2015-12-31|\n|            15|         17|     3|     2015-12-31|\n|            16|         18|     4|     2015-12-31|\n|            17|         19|     1|     2015-12-31|\n|            18|         20|     3|     2015-12-31|\n|            19|         21|     1|     2015-12-31|\n|            20|         23|     4|     2015-12-31|\n+--------------+-----------+------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Transform the data\n",
    "performance = performance \\\n",
    "    .withColumn(\"performance_id\", col(\"performance_id\").cast(IntegerType())) \\\n",
    "    .withColumn(\"employee_id\", col(\"employee_id\").cast(IntegerType())) \\\n",
    "    .withColumn(\"rating\", col(\"rating\").cast(IntegerType())) \\\n",
    "    .withColumn(\"perfomance_date\", to_date(col(\"perfomance_date\"), \"MM/dd/yyyy\"))\n",
    "\n",
    "# Print the schema after transformation\n",
    "performance.printSchema()\n",
    "\n",
    "# Show the data\n",
    "performance.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28846d3c-bf4f-42f4-816c-7bd9476fd730",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------------------+----------------+-------------+---------+-------+-----------+-------------+--------+-----+\n|employee_id|               name|date_of_employement|termination_date|department_id|gender_id|race_id|marriage_id|date_of_birth|pay_rate|level|\n+-----------+-------------------+-------------------+----------------+-------------+---------+-------+-----------+-------------+--------+-----+\n|          1|         Nash Kay K|         2015-01-01|      2017-05-29|            8|     true|      1|          1|   1987-02-12|    null|    1|\n|          2|  Simpson Raymond X|         2015-01-01|            null|            7|    false|      4|          2|   1982-11-15|    null|    1|\n|          3|       Devlin Kay M|         2015-06-10|      2019-06-24|           10|     true|      5|          3|   1986-02-22|    null|    1|\n|          4| Massie Elisabeth V|         2015-01-01|            null|            4|     true|      2|          4|   1977-05-25|    null|    1|\n|          5|   Mead Catherine N|               null|      2020-07-15|            8|     true|      1|          1|   1995-04-04|    null|    2|\n|          6|     Wright Linda K|         2015-11-07|      2022-06-18|           10|     true|      2|          3|   1989-02-13|    null|    2|\n|          7|      Gibbs David A|         2015-04-12|      2018-10-08|            3|    false|      4|          7|   1983-05-06|    null|    1|\n|          8|      Shute Sarah F|         2015-01-01|            null|            9|     true|      2|          8|   1986-04-11|    null|    1|\n|          9|   Woolley Paulin P|         2015-01-01|      2015-02-22|            4|    false|      4|          4|   1995-01-06|    null|    2|\n|         10| Woodbridge Irene A|         2015-09-05|      2022-06-15|            1|     true|      5|         10|   1985-06-21|    null|    1|\n|         11|Richardson Bhavna C|         2015-11-05|      2021-09-29|            9|     true|      2|          8|   1978-07-31|    null|    2|\n|         12|      Lamb Daniel U|               null|            null|            6|    false|      5|         12|   1975-02-03|    null|    1|\n|         13|   Seabrook Sonia S|         2015-01-01|            null|            1|     true|      2|         10|   1990-12-17|    null|    2|\n|         14|     Powell David V|               null|            null|            6|    false|      5|         12|   1978-05-02|    null|    2|\n|         15|   Nielson Paulin V|               null|            null|            4|    false|      2|          4|   1974-11-19|    null|    2|\n|         16|   Goddard Joanna R|         2015-01-01|            null|            7|     true|      5|          2|   1972-08-27|    null|    2|\n|         17|      Webb Steven C|         2015-08-10|            null|            3|    false|      2|          7|   1987-10-26|    null|    2|\n|         18| Channing Francis C|               null|            null|            7|    false|      5|          2|   1968-01-31|    null|    2|\n|         19|    Richards Iris K|         2015-01-01|            null|            9|     true|      5|          8|   1968-10-03|    null|    2|\n|         20|     Ewart Amanda B|         2015-01-01|            null|            7|     true|      4|          2|   1981-02-12|    null|    2|\n+-----------+-------------------+-------------------+----------------+-------------+---------+-------+-----------+-------------+--------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6124d7d2-9e05-4def-9779-932c0ba8f595",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- date_of_employement: date (nullable = true)\n |-- termination_date: date (nullable = true)\n |-- department_id: integer (nullable = true)\n |-- gender_id: boolean (nullable = true)\n |-- race_id: integer (nullable = true)\n |-- marriage_id: integer (nullable = true)\n |-- date_of_birth: date (nullable = true)\n |-- pay_rate: integer (nullable = true)\n |-- level: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "employee.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "051bc3b5-f7cf-4bfe-aec2-3b6034547124",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employee = employee \\\n",
    "    .withColumn(\"employee_id\", col(\"employee_id\").cast(IntegerType())) \\\n",
    "    .withColumn(\"gender_id\", col(\"gender_id\").cast(BooleanType())) \\\n",
    "    .withColumn(\"date_of_employement\", to_date(col(\"date_of_employement\"), \"MM/dd/yyyy\")) \\\n",
    "    .withColumn(\"race_id\", col(\"race_id\").cast(IntegerType())) \\\n",
    "    .withColumn(\"marriage_id\", col(\"marriage_id\").cast(IntegerType())) \\\n",
    "    .withColumn(\"pay_rate\", col(\"pay_rate\").cast(IntegerType())) \\\n",
    "    .withColumn(\"level\", col(\"level\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caabc7fd-7ba7-46a5-8d2e-3ff6dd89f348",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- act_id: string (nullable = true)\n |-- action_id: string (nullable = true)\n |-- employee_id: string (nullable = true)\n |-- date_of_effective: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "action.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65027a92-fb4f-4431-858e-ac5e41a55b61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "action = action \\\n",
    "    .withColumn(\"act_id\", col(\"act_id\").cast(IntegerType())) \\\n",
    "    .withColumn(\"action_id\", col(\"action_id\").cast(IntegerType())) \\\n",
    "    .withColumn(\"employee_id\", col(\"employee_id\").cast(IntegerType())) \\\n",
    "    .withColumn(\"date_of_effective\", to_date(col(\"date_of_effective\"), \"M/d/yyyy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf0d1c3-9812-4427-98a4-6800ae3bdc7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------------------+----------------+-------------+---------+-------+-----------+-------------+--------+-----+\n|employee_id|               name|date_of_employement|termination_date|department_id|gender_id|race_id|marriage_id|date_of_birth|pay_rate|level|\n+-----------+-------------------+-------------------+----------------+-------------+---------+-------+-----------+-------------+--------+-----+\n|          1|         Nash Kay K|         2015-01-01|      2017-05-29|            8|     true|      1|          1|   1987-02-12|    null|    1|\n|          2|  Simpson Raymond X|         2015-01-01|            null|            7|    false|      4|          2|   1982-11-15|    null|    1|\n|          3|       Devlin Kay M|         2015-06-10|      2019-06-24|           10|     true|      5|          3|   1986-02-22|    null|    1|\n|          4| Massie Elisabeth V|         2015-01-01|            null|            4|     true|      2|          4|   1977-05-25|    null|    1|\n|          5|   Mead Catherine N|               null|      2020-07-15|            8|     true|      1|          1|   1995-04-04|    null|    2|\n|          6|     Wright Linda K|         2015-11-07|      2022-06-18|           10|     true|      2|          3|   1989-02-13|    null|    2|\n|          7|      Gibbs David A|         2015-04-12|      2018-10-08|            3|    false|      4|          7|   1983-05-06|    null|    1|\n|          8|      Shute Sarah F|         2015-01-01|            null|            9|     true|      2|          8|   1986-04-11|    null|    1|\n|          9|   Woolley Paulin P|         2015-01-01|      2015-02-22|            4|    false|      4|          4|   1995-01-06|    null|    2|\n|         10| Woodbridge Irene A|         2015-09-05|      2022-06-15|            1|     true|      5|         10|   1985-06-21|    null|    1|\n|         11|Richardson Bhavna C|         2015-11-05|      2021-09-29|            9|     true|      2|          8|   1978-07-31|    null|    2|\n|         12|      Lamb Daniel U|               null|            null|            6|    false|      5|         12|   1975-02-03|    null|    1|\n|         13|   Seabrook Sonia S|         2015-01-01|            null|            1|     true|      2|         10|   1990-12-17|    null|    2|\n|         14|     Powell David V|               null|            null|            6|    false|      5|         12|   1978-05-02|    null|    2|\n|         15|   Nielson Paulin V|               null|            null|            4|    false|      2|          4|   1974-11-19|    null|    2|\n|         16|   Goddard Joanna R|         2015-01-01|            null|            7|     true|      5|          2|   1972-08-27|    null|    2|\n|         17|      Webb Steven C|         2015-08-10|            null|            3|    false|      2|          7|   1987-10-26|    null|    2|\n|         18| Channing Francis C|               null|            null|            7|    false|      5|          2|   1968-01-31|    null|    2|\n|         19|    Richards Iris K|         2015-01-01|            null|            9|     true|      5|          8|   1968-10-03|    null|    2|\n|         20|     Ewart Amanda B|         2015-01-01|            null|            7|     true|      4|          2|   1981-02-12|    null|    2|\n+-----------+-------------------+-------------------+----------------+-------------+---------+-------+-----------+-------------+--------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d82039fd-b4a4-4616-ad31-d81c3efc7a2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-----------+-----------------+\n|act_id|action_id|employee_id|date_of_effective|\n+------+---------+-----------+-----------------+\n|     1|       10|          1|       2015-01-01|\n|     2|       10|          2|       2015-01-01|\n|     3|       10|          3|       2015-10-06|\n|     4|       10|          4|       2015-01-01|\n|     5|       10|          5|       2015-05-22|\n|     6|       10|          6|       2015-07-11|\n|     7|       10|          7|       2015-12-04|\n|     8|       10|          8|       2015-01-01|\n|     9|       10|          9|       2015-01-01|\n|    10|       10|         10|       2015-05-09|\n|    11|       10|         11|       2015-05-11|\n|    12|       10|         12|       2015-12-24|\n|    13|       10|         13|       2015-01-01|\n|    14|       10|         14|       2015-07-26|\n|    15|       10|         15|       2016-03-13|\n|    16|       10|         16|       2015-01-01|\n|    17|       10|         17|       2015-10-08|\n|    18|       10|         18|       2015-03-21|\n|    19|       10|         19|       2015-01-01|\n|    20|       10|         20|       2015-01-01|\n+------+---------+-----------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "action.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b2b606b-335b-4a1d-8dfb-21121747d5fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------+---------------+\n|performance_id|employee_id|rating|perfomance_date|\n+--------------+-----------+------+---------------+\n|             1|          1|     1|     2015-12-31|\n|             2|          2|     2|     2015-12-31|\n|             3|          3|     2|     2015-12-31|\n|             4|          4|     4|     2015-12-31|\n|             5|          5|     2|     2015-12-31|\n|             6|          6|     5|     2015-12-31|\n|             7|          7|     3|     2015-12-31|\n|             8|          8|     4|     2015-12-31|\n|             9|         10|     4|     2015-12-31|\n|            10|         11|     3|     2015-12-31|\n|            11|         12|     4|     2015-12-31|\n|            12|         13|     3|     2015-12-31|\n|            13|         14|     4|     2015-12-31|\n|            14|         16|     3|     2015-12-31|\n|            15|         17|     3|     2015-12-31|\n|            16|         18|     4|     2015-12-31|\n|            17|         19|     1|     2015-12-31|\n|            18|         20|     3|     2015-12-31|\n|            19|         21|     1|     2015-12-31|\n|            20|         23|     4|     2015-12-31|\n+--------------+-----------+------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "performance.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19bce3b7-50e9-4a27-b3bb-b6da8eace899",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employee.repartition(1).write.mode(\"overwrite\").option(\"header\",'true').csv(\"/mnt/hrdata/transformed-data/employee\")\n",
    "action.repartition(1).write.mode(\"overwrite\").option(\"header\",'true').csv(\"/mnt/hrdata/transformed-data/action\")\n",
    "performance.repartition(1).write.mode(\"overwrite\").option(\"header\",'true').csv(\"/mnt/hrdata/transformed-data/performance\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 25429875552994,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "HR_Data_Transformation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
